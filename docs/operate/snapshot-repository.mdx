---
sidebar_position: 11
description: "Strategies for backing up and restoring the Restate data store"
---

import Admonition from '@theme/Admonition';

# Snapshot repository

Restate workers can be configured to periodically produce snapshots of their partition state. Snapshots act as a form of backup and allow nodes that had not previously served a partition to bootstrap a copy of its state. Without snapshots, rebuilding a partition processor would require the full replay of the partition's log. Replaying the log might take a long time or even be impossible if the log was trimmed.

Restate clusters should always be configured with a snapshot destination in order to allow nodes to efficiently share partition state. Restate currently supports using Amazon S3 (or another API-compatible object store) as a shared snapshot repository. To set up a snapshot destination, update your server configuration as follows:

```toml
[worker.snapshots]
destination = "s3://snapshots-bucket/cluster-prefix"
snapshot-interval-num-records = 10000
```

This enables automated periodic snapshots to be written to the specified bucket. You can also trigger snapshot creation manually using the [`restatectl`](/operate/control) `create-snapshot` command. We recommend testing the snapshot configuration by requesting a snapshot and examining the contents of the bucket. You should see a new prefix with each partition's id, and a `latest.json` file pointing to the most recent snapshot.

No additional configuration is required to enable restoring snapshots. When partition processors first start up, and no local partition state is found, the processor will attempt to restore the latest snapshot from the repository. This allows for efficient bootstrapping of additional partition workers.

<Admonition type="note" title="Experimenting with snapshots without an object store">
    For testing purposes, you can also use the `file://` protocol to publish snapshots to a local directory. This is mostly useful when experimenting with multi-node configurations on a single machine. The `file` provider does not support conditional updates, which makes it unsuitable for potentially contended operation.
</Admonition>

For S3 bucket destinations, Restate will use the AWS credentials available from the environment, or the configuration profile specified by `AWS_PROFILE` environment variable, falling back to the default AWS profile.

## Log trimming and snapshots

In a distributed environment, the shared log is the mechanism for replicating partition state among nodes. Therefore it is critical to ensure that all cluster members can get all the relevant updates. This requirement is at odds with an immutable log growing unboundedly. Snapshots enable log trimming in clusters. When partition processors successfully publish a snapshot, they update their "archived" log sequence number (LSN). This reflects the position in the log at which the snapshot was taken.

By default, Restate will attempt to trim logs once an hour which you can override or disable in the server configuration:

```toml
[admin]
log-trim-interval = "1h"
```

This interval is only the check and not a guarantee that logs will be trimmed. Restate will automatically determine the appropriate safe trim point for each partition's log.

If replicated logs are in use in a clustered environment, the log safe trim point will be determined based on the archived LSN. If a snapshot repository is not configured, then archived LSNs are not reported. Instead, the safe trim point will be determined by the smallest reported persisted LSN across all known processors for the given partition. Single-node local-only logs are also trimmed based on the partitions' persisted LSNs.

The presence of any dead nodes in a cluster will cause trimming to be suspended for all partitions, unless a snapshot repository is configured. This is because we can not know what partitions may reside on the unreachable nodes, which will become stuck when the node comes back.

When a node starts up with pre-existing partition state and finds that the partition's log has been trimmed to a point beyond the most recent locally-applied LSN, the node will attempt to download the latest snapshot from the configured repository. If a suitable snapshot is available, the processor will re-bootstrap its local state and resume applying the log.

<Admonition type="note" title="Handling log trim gap startup errors">
    If you observe repeated `Shutting partition processor down because it encountered a trim gap in the log.` errors in the Restate server log, that is an indication that a processor is failing to start up due to missing log records. In order to record, you must ensure that a snapshot repository is correctly configured and accessible from the the node experiencing errors. If no snapshots were taken previously, you will need to configure another node which has the necessary state, to first publish a snapshot for the affected partition(s).
</Admonition>
