---
title: "Migrating a single-node to a multi-node deployment"
description: "Learn how to migrate from a single node to a multi-node cluster using the replicated loglet and replicated metadata server."
pagination_next: null
pagination_prev: null
---

import Admonition from '@theme/Admonition';
import {Step} from "../../src/components/Stepper";

# Migrating a single-node to a multi-node deployment

<Admonition type="warning">
    You should [upgrade](/operate/upgrading) your single-node deployment to the latest Restate version before adding more nodes.
</Admonition>

This guide shows how to scale an existing single-node deployment to a multi-node cluster. It assumes you have a running single-node Restate server that is running the replicated loglet and replicated metadata server, which are enabled by default in Restate >= v1.4.0.

Older versions of Restate ({'<='} v1.3.2) use the local loglet and local metadata server by default. The local loglet and local metadata server are suitable for development and single-node deployments. We recommend using the replicated loglet and replicated metadata server to ensure high availability and durability. They are also required for multi-node clusters. Starting with version v1.4.0, existing logs and metadata will be automatically migrated to the replicated equivalents.

<Step stepLabel="1" title="Verify that node is running the replicated metadata server">

Check that the metadata service is running using the [`restatectl`](/operate/clusters#controlling-clusters-with-restatectl) tool.

```shell
restatectl metadata-server list-servers
```

You should see a single member node providing metadata service:

```
Metadata service
 NODE  STATUS  VERSION  LEADER  MEMBERS  APPLIED  COMMITTED  TERM  LOG-LENGTH  SNAP-INDEX  SNAP-SIZE
 N1    Member  v1       N1      [N1]     2        2          2     1           1           6.7 KiB
```

If you see the node as unreachable with an error reason of "Unimplemented", verify that you are running the latest version. The older local metadata server is no longer supported in Restate v1.4.0 and newer.

</Step>

<Step stepLabel="2" title="Verify that node is running the replicated loglet">

The default configuration is cluster-ready. However, if you have explicitly specified server roles in configuration, you should make sure these include the `log-server` role. Similarly, if you have explicitly set the loglet provider to be `local`, you should remove this. While the local loglet is still supported, the default type is `replicated` starting from v1.4.0. If you have a configuration file and would like to make these settings explicit, it should contain the following:

```toml restate.toml
roles = [
    "worker",
    "admin",
    "metadata-server",
    "log-server",                # needed for replicated loglet
    "http-ingress",
]

[bifrost]
default-provider = "replicated"  # default
```

Confirm that cluster configuration uses the replicated loglet as the default log provider.

```shell
restatectl config get
```

In the default configuration you should expect to see:

```
‚öôÔ∏è Cluster Configuration
‚îú Number of partitions: 24
‚îú Partition replication: {node: 1}
‚îî Logs Provider: replicated
 ‚îú Log replication: {node: 1}
 ‚îî Nodeset size: 1
```

You can confirm that the type of logs in use by the server using the command:

```shell
restatectl logs list
```

If you have enabled the `log-server` role and left the default provider unset (or set it to `replicated`), and still do not see the cluster configuration you can change the cluster log configuration using:

```shell
restatectl config set --log-provider replicated --log-replication 1
```

This command sets the default log provider to `replicated` with a default replication of `1`.
As long as you have a single-node deployment, you must set the replication to `1`.
Otherwise, the server will become unavailable because it cannot provision the new log segments.

</Step>

<Step stepLabel="3" title="Configure snapshot repository">

If you plan to extend your single-node deployment to a multi-node deployment, you also need to [configure the snapshot repository](/operate/snapshots).
This allows new nodes to join the cluster by restoring the latest snapshot.

```toml restate.toml
[worker.snapshots]
destination = "s3://snapshots-bucket/cluster-prefix"
```

</Step>

<Step stepLabel="4" title="Create snapshots to allow other nodes to join">

For other nodes to join, you need to snapshot every partition because the local loglet is not accessible from other nodes.
Run the following command to create a snapshot for each partition.

```shell
restatectl snapshots create --trim-log
```

Note that this also instructs Restate to trim the logs after partition state has been successfully published to the snapshot repository. This ensures that the logs no longer reference historic local loglets that may have existed on the original node.

</Step>

<Step stepLabel="5" title="Turn a single-node into a multi-node deployment">

To add more nodes to your cluster, you need to start new Restate servers with the same `cluster-name` and a metadata client with the address of the existing node.

```toml restate.toml
cluster-name = "my-cluster"

[metadata-client]
addresses = ["http://metadata-node.cluster:5122"]
```

Metadata is critical to the operation of your cluster and we recommend that you run the `metadata-server` role on additional nodes. Make the cluster metadata service resilient to node failures by specifying the full list of metadata servers on all cluster nodes.

```toml restate.toml
[metadata-client]
addresses = ["http://node1.cluster:5122", "http://node2.cluster:5122", "http://node3.cluster:5122"]
```

</Step>

<Step stepLabel="6" title="Verify that your cluster consists of multiple nodes">

If everything is set up correctly, you should see the new nodes in the cluster status.

```shell
restatectl status
```

</Step>

<Step end={true} stepLabel="üéâ" title="Congratulations, you migrated your single-node deployment to a multi-node deployment!"/>


Here are some next step for you to try:

- Try [growing your cluster](/operate/clusters#growing-the-cluster) to tolerate node failures
